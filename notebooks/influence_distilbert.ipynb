{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/fabio/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 295.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/fabio/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/fabio/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c1eaa46e94dfbfd3.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = (\n",
    "    imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
    ")\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive probability: 96.0169792175293%\n",
      "Negative probability: 3.9830222725868225%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"assemblyai/distilbert-base-uncased-sst2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"assemblyai/distilbert-base-uncased-sst2\"\n",
    ")\n",
    "\n",
    "tokenized_segments = tokenizer(\n",
    "    [\n",
    "        \"AssemblyAI is the best speech-to-text API for modern developers with performance being second to none!\"\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "tokenized_segments_input_ids, tokenized_segments_attention_mask = (\n",
    "    tokenized_segments.input_ids,\n",
    "    tokenized_segments.attention_mask,\n",
    ")\n",
    "model_predictions = F.softmax(\n",
    "    model(\n",
    "        input_ids=tokenized_segments_input_ids,\n",
    "        attention_mask=tokenized_segments_attention_mask,\n",
    "    )[\"logits\"],\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "print(\"Positive probability: \" + str(model_predictions[0][1].item() * 100) + \"%\")\n",
    "print(\"Negative probability: \" + str(model_predictions[0][0].item() * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/fabio/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-6582379d1c4310e2.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/fabio/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c700964b93e7a374.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encodings[idx])\n",
    "        y = torch.tensor(self.labels[idx])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImdbDataset(tokenized_train[\"input_ids\"], tokenized_train[\"label\"])\n",
    "test_dataset = ImdbDataset(tokenized_test[\"input_ids\"], tokenized_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kj/nzr59d991gj0y17m_d24hfyr0000gn/T/ipykernel_85502/2951157367.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model.base_model(torch.tensor(train_dataset[0][0]).unsqueeze(0), torch.tensor(train_dataset[0][0]).unsqueeze(0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1542,  0.0906,  1.4949,  ...,  0.0321,  1.1469, -0.2308],\n",
       "         [-0.2363,  0.5633, -0.1705,  ..., -0.2990,  0.9639,  0.7347],\n",
       "         [-0.5188,  0.6324,  0.1162,  ..., -0.4970,  0.6356,  1.4636],\n",
       "         ...,\n",
       "         [ 0.3062,  0.1396,  0.1965,  ...,  0.0643,  0.4370, -0.0995],\n",
       "         [-0.0399, -0.2684,  0.6871,  ...,  0.2505,  0.1654,  0.0542],\n",
       "         [-0.0114, -0.2121,  0.7543,  ...,  0.3027,  0.2304, -0.0872]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model(\n",
    "    torch.tensor(train_dataset[0][0]).unsqueeze(0),\n",
    "    torch.tensor(train_dataset[0][0]).unsqueeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kj/nzr59d991gj0y17m_d24hfyr0000gn/T/ipykernel_85502/561957760.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = F.cross_entropy(model(torch.tensor(train_dataset[0][0]).unsqueeze(0), torch.tensor(train_dataset[0][0]).unsqueeze(0))['logits'], torch.tensor(train_dataset[0][1]).unsqueeze(0))\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(\n",
    "    model(\n",
    "        torch.tensor(train_dataset[0][0]).unsqueeze(0),\n",
    "        torch.tensor(train_dataset[0][0]).unsqueeze(0),\n",
    "    )[\"logits\"],\n",
    "    torch.tensor(train_dataset[0][1]).unsqueeze(0),\n",
    ")\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6890e-06)\n",
      "tensor(1.3351e-05)\n",
      "tensor(1.4605)\n",
      "tensor(-2.6148)\n",
      "tensor(-1.6427)\n",
      "tensor(0.0928)\n",
      "tensor(-15.6059)\n",
      "tensor(-5.8470e-08)\n",
      "tensor(0.4710)\n",
      "tensor(0.5902)\n",
      "tensor(-3.8147e-06)\n",
      "tensor(2.3842e-07)\n",
      "tensor(-0.9682)\n",
      "tensor(0.5145)\n",
      "tensor(11.0682)\n",
      "tensor(-1.1260)\n",
      "tensor(-1.5259e-05)\n",
      "tensor(2.8312e-07)\n",
      "tensor(-3.1945)\n",
      "tensor(-0.2525)\n",
      "tensor(-3.9610)\n",
      "tensor(0.1884)\n",
      "tensor(4.3107)\n",
      "tensor(1.1079e-07)\n",
      "tensor(-35.6873)\n",
      "tensor(1.7198)\n",
      "tensor(7.6294e-06)\n",
      "tensor(1.1921e-07)\n",
      "tensor(-0.7906)\n",
      "tensor(-3.1745)\n",
      "tensor(-8.5136)\n",
      "tensor(0.3550)\n",
      "tensor(-0.0001)\n",
      "tensor(8.3447e-07)\n",
      "tensor(-5.2181)\n",
      "tensor(-36.4425)\n",
      "tensor(-2.2278)\n",
      "tensor(0.1347)\n",
      "tensor(1.0476)\n",
      "tensor(-7.5299e-08)\n",
      "tensor(76.2288)\n",
      "tensor(-3.8812)\n",
      "tensor(-9.5367e-07)\n",
      "tensor(8.3447e-07)\n",
      "tensor(-5.1421)\n",
      "tensor(-10.4402)\n",
      "tensor(-51.4626)\n",
      "tensor(1.9585)\n",
      "tensor(-1.5259e-05)\n",
      "tensor(-4.1723e-07)\n",
      "tensor(-1.4264)\n",
      "tensor(-14.5551)\n",
      "tensor(-0.2876)\n",
      "tensor(0.0015)\n",
      "tensor(0.1487)\n",
      "tensor(-1.0553e-07)\n",
      "tensor(-11.1569)\n",
      "tensor(0.6947)\n",
      "tensor(4.1723e-06)\n",
      "tensor(-3.5763e-07)\n",
      "tensor(-3.0723)\n",
      "tensor(2.8420)\n",
      "tensor(9.1707)\n",
      "tensor(-0.1464)\n",
      "tensor(0.0001)\n",
      "tensor(-6.8545e-07)\n",
      "tensor(-0.6823)\n",
      "tensor(6.3952)\n",
      "tensor(-4.1974)\n",
      "tensor(0.1477)\n",
      "tensor(0.9981)\n",
      "tensor(1.0599e-07)\n",
      "tensor(-24.7237)\n",
      "tensor(0.8812)\n",
      "tensor(-1.5497e-06)\n",
      "tensor(2.2352e-07)\n",
      "tensor(0.0438)\n",
      "tensor(-1.0393)\n",
      "tensor(-31.0002)\n",
      "tensor(0.9036)\n",
      "tensor(0.0001)\n",
      "tensor(-4.0978e-07)\n",
      "tensor(-1.3644)\n",
      "tensor(-3.0720)\n",
      "tensor(1.0456)\n",
      "tensor(-0.0384)\n",
      "tensor(-1.0836)\n",
      "tensor(8.2734e-08)\n",
      "tensor(-6.3896)\n",
      "tensor(0.2369)\n",
      "tensor(2.3842e-07)\n",
      "tensor(1.4901e-08)\n",
      "tensor(-0.0829)\n",
      "tensor(0.0020)\n",
      "tensor(-0.2208)\n",
      "tensor(0.0050)\n",
      "tensor(1.9073e-05)\n",
      "tensor(-1.0058e-07)\n",
      "tensor(-0.8166)\n",
      "tensor(0.1634)\n"
     ]
    }
   ],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    print(param.grad.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydvl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfluence\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_influences\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydvl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfluence\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchTwiceDifferentiable\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ekfac_train_influences \u001b[39m=\u001b[39m compute_influences(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     TorchTwiceDifferentiable(model_logits, F\u001b[39m.\u001b[39;49mcross_entropy),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     training_data\u001b[39m=\u001b[39mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     test_data\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     influence_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mup\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     inversion_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mekfac\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     hessian_regularization\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     progress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_distilbert.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/torch/torch_differentiable.py:82\u001b[0m, in \u001b[0;36mTorchTwiceDifferentiable.__init__\u001b[0;34m(self, model, loss)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     78\u001b[0m     model: nn\u001b[39m.\u001b[39mModule,\n\u001b[1;32m     79\u001b[0m     loss: Callable[[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39;49mtraining:\n\u001b[1;32m     83\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassed model not in evaluation mode. This can create several issues in influence \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcomputation, e.g. due to batch normalization. Please call model.eval() before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcomputing influences.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'training'"
     ]
    }
   ],
   "source": [
    "from pydvl.influence import compute_influences\n",
    "from pydvl.influence.torch import TorchTwiceDifferentiable\n",
    "\n",
    "ekfac_train_influences = compute_influences(\n",
    "    TorchTwiceDifferentiable(model_logits, F.cross_entropy),\n",
    "    training_data=train_dataloader,\n",
    "    test_data=test_dataloader,\n",
    "    influence_type=\"up\",\n",
    "    inversion_method=\"ekfac\",\n",
    "    hessian_regularization=0.1,\n",
    "    progress=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydvl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
